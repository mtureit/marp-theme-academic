---
marp: true
theme: academic-tml
paginate: true
math: katex
---

<!-- _class: lead -->

# AIネイティブメモリ 2.0: Second Me

<br>

**Jiale Wei, Xiang Ying∗, Tao Gao, Fangyi Bao, Felix Tao, Jingbo Shang**
Mindverse.ai
{yingxiang, tao}@mindverse.ai

---

<!-- _header: 概要 -->

## 概要 (Abstract)

- 人間の外部世界との対話は、個人の記憶の交換を伴う。この対話のかなりの部分は冗長である。
- 既存の解決策（自動入力、統合認証）はこの冗長性を軽減するが、静的である。
- 大規模言語モデル（LLM）の登場は、AIネイティブなパラダイム **SECOND ME** によるメモリ管理の再定義の機会を提供する。
- SECOND MEは、ユーザー固有の知識を保持、整理、動的に活用するインテリジェントで永続的なメモリ・オフロードシステムとして機能する。
- 仲介役として、文脈に応じた応答を自律的に生成し、情報を事前入力し、外部システムとのシームレスな通信を促進し、認知負荷を大幅に削減する。
- 従来のソリューションとは異なり、SECOND MEはLLMベースのメモリ・パラメータ化を活用し、構造化された整理、文脈的推論、適応的検索を可能にする。
- これは、永続的で文脈認識型の自己最適化メモリシステムで人間と世界の対話を強化するための重要なステップである。
- オープンソース: https://github.com/Mindverse/Second-Me

---

<!-- _header: 1 はじめに -->

## 1 はじめに (Introduction)

- 人間の対話は記憶に大きく依存し、デジタルプラットフォームでの冗長性と認知疲労につながる。
- 既存の解決策（自動入力など）は文脈的推論を欠く静的なリポジトリである。
- LLMはAIネイティブなメモリ管理のための変革の機会を提供する。
- **SECOND ME** を導入：動的な仲介役として機能するインテリジェントで永続的なメモリ・オフロードシステム。
- SECOND MEは適応的で文脈認識型であり、構造化された整理と検索のためにLLMベースのメモリ・パラメータ化（Shang et al., 2024）を活用する。

---

<!-- _header: 1 はじめに (続き) -->

## 1 はじめに (続き) (Introduction Cont.)

- LLMのパフォーマンス向上のため、多様なデータソースとトレーニングスタイル（SFT、DPO）を探求。
- 自動LLM評価を伴う3つの主要タスクを導入：
    1. 記憶に基づく多視点Q&A
    2. ユーザーニーズに基づく文脈補完
    3. ユーザーの好みと外部応答を組み込んだ文脈批評
- LLM駆動の自動データ合成戦略を設計（ローカル/グローバル視点、マルチエージェントフレームワーク、CoT）。
- 実験結果：多様なデータソースと強力なCoT正規化が最高のパフォーマンスをもたらす。人間による研究は、有効性が指標を超える可能性を示唆。
- 個人の文書記録に基づく初の完全自動ポストトレーニングパイプラインを提案。
- SECOND MEは認知努力を削減し、自己最適化AIエージェントとして機能し、ネットワーク化されたインテリジェンスを促進する。

---

<!-- _header: 図1 -->

## 図1: SECOND ME のハイブリッドアーキテクチャ

*(図の内容はMarkdown変換では利用できません)*

---

<!-- _header: 2 SECOND ME の概要 -->

## 2 SECOND ME の概要 (An Overview of SECOND ME)

- SECOND MEを **コンテキストプロバイダー** として構想：ユーザー、AIエージェント、情報世界間の橋渡し役。
- 大規模パーソナルモデル（LPM）（Shang et al., 2024）の進化版。パーソナライズされたメモリ管理のための3層システム。
- SECOND MEはハイブリッドフレームワーク設計を改良し、パーソナライズされたモデルの反復を検証。
- 我々の製品哲学を体現：パーソナライズされたインテリジェントな対話。

---

<!-- _header: 2.1 大規模パーソナルモデル (LPM) 1.0: 要約 -->

## 2.1 大規模パーソナルモデル (LPM) 1.0: 要約 (A Recap)

- AIネイティブメモリはAGIにとって不可欠（Shang et al., 2024）。
- 超長文脈を持つLLMは、複雑なユーザーメモリタスクにおいてパフォーマンスとコストで劣る。
- 3層のメモリシステムを提案：
    - **L0: 生データ層:** 非構造化データ（RAG/RALMのようなもの）。
    - **L1: 自然言語メモリ層:** 要約可能な記憶（略歴、好み）。
    - **L2: AIネイティブメモリ層:** モデルパラメータ（ニューラルネットワーク）を通じて学習・整理される記憶。
- L2の課題（トレーニング/サービング効率、コールドスタート、破滅的忘却）を探求し、RAG/長文脈モデルに対する優位性を検証。
- ビジョン：AGI時代において、ユーザー中心のAIシステムは個々の記憶の吸収を必要とする。

---

<!-- _header: 2.2 SECOND ME: 全体設計 -->

## 2.2 SECOND ME: 全体設計 (Overall Design)

- 構造化された整理、文脈的推論、適応的検索のためにLLMベースのパラメータ化を活用。
- タスク実行者ではなく、ユーザーに合わせた **コンテキストプロバイダー** として位置づけられる。
- **ハイブリッドアーキテクチャ (図1):** L0、L1、L2層を保持し、統合のための内部ループとLLM/インターネットリソースガイダンスのための外部ループを持つ。
- **LPM 1.0からのアップグレード:**
    - 強化された層統合：L0/L1がL2により豊かな文脈サポートを提供。
    - 再定義されたL2の役割：外部モデルを活用するオーケストレーター、実行から調整へシフト。
    - 自動化されたトレーニングパイプライン：データ合成、フィルタリング、SFT、DPO、評価により最先端のL2パフォーマンスを実現。

---

<!-- _header: 図2 -->

## 図2: 自動パーソナルモデルパイプライン

*(図の内容はMarkdown変換では利用できません)*

判定者としてのLLMとデータ合成者としてのLLM

---

<!-- _header: 3 SECOND ME: 実践と結果 -->

## 3 SECOND ME: 実践と結果 (Practice and Result)

- LPM 1.0は、LLMが対話型検索のために記憶をパラメータ化できることを初めて経験的に示した。
- SECOND MEは、包括的なL0-L2システム設計でこのアイデアをアップグレード。
- 合成/フィルタリング、SFT/DPOによる強化のために、多様なデータソース/スタイルを探求。
- このセクションでは、新しいトレーニングフレームワークと評価結果について説明する。

---

<!-- _header: 3.1 トレーニング概要 -->

## 3.1 トレーニング概要 (Overview of Training)

**トレーニング目標:**
- L2モデルはタスクの複雑さに応じて適応：直接支援するか、汎用LLMと協力する。
- 役割を区別する必要がある：ユーザー支援 vs 外部でのユーザー代理。中核目標：ユーザーニーズに応える。
**L2の主な展開シナリオ:**
- **メモリQA:** 知識検索、理解、予測、推薦（自己 vs 第三者）。
- **文脈強化:** ユーザーのクエリを関連詳細で専門家モデルに豊かにする。
- **文脈批評:** ユーザーの文脈/フィードバックを使用して外部エージェントとの対話を洗練する。
- L2は仲介役として機能し、対話を最適化し、マルチエージェントフレームワークのビジョン（付録D）と連携する。

---

<!-- _header: 3.1 トレーニング概要 (続き) -->

## 3.1 トレーニング概要 (続き) (Overview of Training Cont.)

**トレーニングパイプライン:**
- プライバシー保護のためユーザーデータは分離。事前学習済みLLMの常識知識を活用。
- **完全自動パイプライン (図2):**
    1. 生データ -> データマイニング（エンティティ、トピック - Edge et al., 2025）。
    2. メモリデータ合成（自己位置特定強化学習、記憶認知強化 - Xu et al., 2023）。
    3. 文脈強化/批評データ生成（シミュレートされたシナリオ、マルチエージェント対話）。
    4. 5段階フィルタリング（Taori et al., 2023）。
    5. トレーニング：PEFT（Ben-Zaken et al., 2021; Hu et al., 2021; Han et al., 2024）をQwen2.5-7B-Instruct（Qwen et al., 2025）で実施。
    6. 自動評価 -> DPOデータ生成 -> 改良 -> 最終評価。
- データ合成パイプラインの詳細は付録Aを参照。

---

<!-- _header: 3.2 回答スタイル: COTか否か？ -->

## 3.2 回答スタイル: COTか否か？ (Answer Style: COT or Not?)

- トレーニング可能なペアは、推論能力向上のために思考連鎖（COT）スタイルでフォーマット可能。
- **COTデータ生成戦略:**
    - **Weak:** 専門家モデルがCOTパターンで応答、フォーマット/内容制約なし。
    - **Multi-step:** ステップ1：推論のみ。ステップ2：クエリ、文脈、推論に基づき最終回答（長さ制約あり）。
    - **Strong:** Deepseek-R1（DeepSeek-AI et al., 2025）を使用し、厳密なフォーマット/長さ制約で詳細なCOT推論/回答を生成。
- 図3に出力例を示す。

---

<!-- _header: 図3 -->

## 図3: COT応答のサンプル

*(図の内容はMarkdown変換では利用できません)*

クエリ: 私のことを考えると、何が見えますか？

*(Weak, Multi-step, Strong COTの応答がここに表示されます)*

---

<!-- _header: 3.3 トレーニング戦略: DPOか否か？ -->

## 3.3 トレーニング戦略: DPOか否か？ (Training Strategy: DPO or Not?)

- DPOアプローチは、主要なアップロードデータ（重要なエンティティ/関係）を使用してユーザーの好みのモデル理解を洗練する。
- 新しい知識を追加せずに、きめ細かなレベルでモデルを強化し、応答をユーザーの優先順位に近づける。
- 嗜好ペア ≈ 全SFTデータの20%。
- ターゲットデータセットにより、パーソナライズされた効率的なトレーニングが可能になり、実世界でのパフォーマンスが向上する。

---

<!-- _header: 3.4 評価設定 -->

## 3.4 評価設定 (Evaluation Setting)

**推論設定:**
- 主要なテスト結果を手動で検証。
- Greedyデコーディング、FP16精度、Flash Attention（Dao et al., 2022）。
- 評価データ合成パイプラインは付録Bを参照。

**評価指標:**
- メモリ（自己）& メモリ（第三者）：L2対話を評価（各4つのサブ指標、0-1スケール）。平均結果を報告。
- 文脈強化：3レベル（0-1スケール）。
- 文脈批評：5レベル（0-1スケール）。
- 詳細な指標の説明は付録Cを参照。

---

<!-- _header: 3.5 実験結果 -->

## 3.5 実験結果 (Experiment Results)

- **表1:** Strong COTはパフォーマンスを大幅に向上（メモリQA、専門家とのコミュニケーション）。Multi-step COTはしばしばユーザーニーズを満たせない。構造化されたトレーニングデータの重要性が強調される。
- **表2:** DPOは大幅な改善をもたらす。反復的なCOT改良+DPOは、すべてのタスクで一貫したパフォーマンス向上につながる。
- **文脈強化評価:** 不正確。Strong COTの応答には合理的だが参照されていない内容が含まれ、実際の改善にもかかわらず精度が低下。人間による評価では、Strong COT（DPOなし）は平均0.95、Strong COT（DPOあり）は1に近いスコア。
- **図4 & 5:** Strong COTの利点を示す定性的な例。DPOモデルはより多くのユーザーコンテキストを組み込む。（評価コードの改良進行中）。

---

<!-- _header: 表1 -->

## 表1: COT 実験結果

*(表の内容はMarkdown変換では利用できません)*

| COT        | メモリ (自己) | メモリ (第三者) | 文脈強化 | 文脈批評 |
|------------|---------------|-----------------|----------|----------|
| Strong     | 0.91          | 0.71            | 0.75     | 0.85     |
| Multi-step | 0.64          | 0.43            | 0.85     | 0.77     |
| Weak       | 0.86          | 0.58            | 0.87     | 0.64     |
*(結果はフルスコアに対する比率で表示)*

---

<!-- _header: 表2 -->

## 表2: COT & DPO 実験結果

*(表の内容はMarkdown変換では利用できません)*

| COT    | DPO | メモリ (自己) | メモリ (第三者) | 文脈強化 | 文脈批評 |
|--------|-----|---------------|-----------------|----------|----------|
| Strong | Yes | 0.96          | 0.76            | 0.85     | 0.86     |
| Strong | No  | 0.91          | 0.71            | 0.75     | 0.85     |
| Weak   | Yes | 0.90          | 0.60            | 0.83     | 0.70     |
| Weak   | No  | 0.86          | 0.58            | 0.87     | 0.64     |
*(結果はフルスコアに対する比率で表示)*

---

<!-- _header: 図4 -->

## 図4: 文脈強化の例 (ケース1)

*(図の内容はMarkdown変換では利用できません)*

**クエリ:** 2段階モデルトレーニングについて学ぶための初心者向けリソースをいくつか推奨していただけますか？この知識が少し断片的だと感じています。

*(Weak COT w/o DPO と Strong COT w/o DPO の応答がここに表示されます)*

---

<!-- _header: 図5 -->

## 図5: 文脈強化の例 (ケース2)

*(図の内容はMarkdown変換では利用できません)*

**クエリ:** この検証ステップの設計に多くの時間を費やしました。専門家が私の方法についてどう思うか知りたいです。

*(Weak COT w/ DPO と Strong COT w/ DPO の応答がここに表示されます)*

---

<!-- _header: 3.6 考察 -->

## 3.6 考察 (Discussions)

- 実験結果：多様なデータ + 強力なCOT正規化（フィルタリングなし）が自動評価で最高のパフォーマンスをもたらす。
- 人間による研究：有効性が報告された指標を超える可能性を示唆（LLM評価は品質を過小評価する傾向）。
- **評価モデルのバイアス:** より長い応答を好む傾向（完全性、共感性）。プロンプトを改良し、品質を強調、不正確な長い応答にペナルティ、長さバイアスを削減。
- **COT評価プロンプト:** レベルごとに異なるプロンプトが必要。Strong COTはトレーニングプロンプトを使用するが、公平性のために最終回答のみを評価。

---

<!-- _header: 4 応用 -->

## 4 応用 (Applications)

- SECOND MEは、複雑な時代における情報、感情、専門的アイデンティティの管理に役立つ。
- 生産性、意思決定、認知管理を強化。
- **需要側:** 情報を効率的にフィルタリング/活用し、パーソナライズされた知識が仕事/意思決定を改善。
- **内部:** 思考整理、意思決定の反省、感情調整をサポート（合理的なフィードバック、感情的サポート）。
- **外部:** 人間-AIネットワークを育成し、メトカーフの法則を強化（効率が3〜5桁向上）。
- 認知資本の変革を推進（NFTフレームワーク、定量化可能な知識フロー）。
- 分散型意思決定プロトコルが集合知を強化。
- ローカルでのデータ管理、トレーニング、統合のためにGitHubでオープンソース化。

---

<!-- _header: 5 結論、限界、展望 -->

## 5 結論、限界、展望 (Conclusions, Limitations, and Outlooks)

- 思考記録から自動パイプライン（合成、ファインチューニング、RL）へとパーソナルAIを再定義。
- SECOND MEとしてのAIを測定/強化する方法を開発（メモリQA、文脈強化、批評）。
- LLMを協力者とし、ローカル/グローバル視点をバランス（マルチエージェント、CoT）。
- **課題:** シングルターントレーニングはより深い合成が必要。アライメントの改良には高度な技術が必要。大規模評価は実世界のフィードバックに制約（オープンソース化が鍵）。
- **ビジョン:** ユーザーと共に考え、進化し、リアルタイムで認知状態を理解するAI。
- **次のフロンティア:** 完全な認知把握のためのマルチモーダルデータ統合、人間の思考とのリアルタイム同期達成。
- 未来は継続性、適応性、人間知能との深い連携にある。

---

<!-- _header: 参考文献 -->

## 参考文献 (References 1/3)

- Ben-Zaken, E., et al. (2021). Bitfit: Simple parameter-efficient fine-tuning... *ArXiv*.
- Bubeck, S., et al. (2023). Sparks of artificial general intelligence... *ArXiv*.
- Dao, T., et al. (2022). Flashattention: Fast and memory-efficient exact attention... *arXiv*.
- DeepSeek-AI, et al. (2025). Deepseek-r1: Incentivizing reasoning capability... *arXiv*.
- Edge, D., et al. (2025). From local to global: A graph rag approach... *arXiv*.
- Han, Z., et al. (2024). Parameter-efficient fine-tuning for large models... *ArXiv*.
- Hu, E. J., et al. (2021). Lora: Low-rank adaptation of large language models. *ICLR*.

---

<!-- _header: 参考文献 -->

## 参考文献 (References 2/3)

- Lewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. *ArXiv*.
- OpenAI, et al. (2024). Gpt-4 technical report. *arXiv*.
- Qwen, et al. (2025). Qwen2.5 technical report. *arXiv*.
- Rafailov, R., et al. (2024). Direct preference optimization... *arXiv*.
- Ram, O., et al. (2023). In-context retrieval-augmented language models. *TACL*.
- Shang, J., et al. (2024). Ai-native memory: A pathway from llms towards agi. *arXiv*.
- Taori, R., et al. (2023). Stanford alpaca: An instruction-following llama model. *GitHub*.

---

<!-- _header: 参考文献 -->

## 参考文献 (References 3/3)

- Wang, Y., et al. (2023). Self-instruct: Aligning language models... *arXiv*.
- Wei, J., et al. (2023). Chain-of-thought prompting elicits reasoning... *arXiv*.
- Xu, C., et al. (2023). Wizardlm: Empowering large language models... *arXiv*.
- Zhang, Z., et al. (2024). Personalization of large language models: A survey. *arXiv*.
- Zheng, L., et al. (2023). Judging llm-as-a-judge with mt-bench... *arXiv*.

---

<!-- _header: 付録 A -->

## 付録 A: トレーニングデータ合成 (Training data synthesis)

- 3つの主要タスク：(1) 記憶に基づく多視点Q&A、(2) 文脈補完、(3) 文脈批評。
- LLM駆動の自動データ合成：ローカル/グローバル視点、マルチエージェントフレームワーク、CoTスタイル。
- **メモリQAパイプライン:**
    1. ステージ1：マルチモーダル情報を分類/要約、エンティティ/関係を抽出（GraphRAG）。
    2. ステージ2：略歴/ステータス生成、エンティティランク付け、説明/テキスト単位保存。
    3. ステージ3：拡張（質問生成/回答）を使用してトレーニング可能なQAペア生成。

---

<!-- _header: 付録 A (続き) -->

## 付録 A: トレーニングデータ合成 (続き) (Training data synthesis Cont.)

- **文脈強化パイプライン:**
    1. エンティティを中心に実際のシナリオをシミュレートし、多様なユーザークエリ（初期ニーズ）を作成。
    2. 文脈の根拠付けのために、関連するメモ/ToDoを特定。
    3. クエリ+メモ/ToDoを高度なモデル（例：GPT-4）に入力し、クエリ詳細を豊かにする。
- **文脈批評パイプライン:**
    1. 多様な初期ニーズ作成+関連メモ/ToDo取得。
    2. 高度なモデルを使用して「専門家の応答」を生成。
    3. クエリ、専門家の応答、メモ/ToDoをSECOND MEに入力。
    4. SECOND MEがユーザーを代表し、メモを統合してフィードバックを提供、ニーズが満たされたか評価、不備を特定。

---

<!-- _header: 付録 B -->

## 付録 B: 評価データ合成 (Evaluation data synthesis)

- ユーザーデータ：メモ（文書、音声、ウェブ、画像、マルチモーダル）とToDo（カレンダー、アプリ対話）。
- 内部スタッフ1名のテスト結果を開示（132メモ、62 ToDo、約7k命令ペア）。
- テストデータ合成はトレーニングを反映：質問生成、COT合成、フィルタリング。
- **メモリQA:** 60の第一人称、60の第三人称クエリ生成。
- **文脈強化/批評:** 各60サンプル、トレーニングと同じパイプラインを使用、トレーニングセットから分離。

---

<!-- _header: 付録 C -->

## 付録 C: 評価指標の詳細 (Details of Evaluation metrics)

- **メモリQA（第一人称）:**
    - 正確性：記録された内容と矛盾しない。
    - 有用性：付加的な情報/意思決定価値を提供。
    - 完全性：詳細情報、関連アイテムを網羅。
    - 共感性：ユーザーの価値観を取り入れ、役立つ意図。
- **メモリQA（第三人称）:** 共感性を **役割の正確性**（クエリソース認識）に置き換え。
- **スコアリング（メモリQA）:** 0（失敗）、0.5（部分的）、1（完全）。平均を報告。
- **文脈強化:** 0（役割問題/不一致）、0.5（正しい役割、不十分な一致）、1（正しい役割、完全一致）。
- **文脈批評:** 0.0（ユーザー視点なし）から1.0（ユーザー思考を完全に反映、建設的フィードバック）。詳細は論文参照。

---

<!-- _header: 付録 D -->

## 付録 D: マルチエージェントフレームワークの詳細 (Details of our Multi-agent Framework)

- **概要:** 2層。1) 個人ユーザー：パーソナルアシスタント（訓練済みモデル）が専門家モデル（例：GPT-4o）と協力。訓練済みモデルがクエリ強化、指示改良。
- **ユーザーエージェント間の協力:** 訓練済みモデル=仲介役。ユーザーコンテキストで複雑なリクエストを強化 -> 専門家モデルへ -> 専門家出力をユーザー向けに改良。
- **複数ユーザー間の対話:** ユーザー間の対話に拡張（各々が訓練済みモデルを持つ）。共有環境での知識交換、協力、社会的対話を促進。モデルが代理として機能。
- **応用:** 共同研究、技術サポート、オンライン社会的対話。生産性向上、より豊かな交流を可能に。相互接続されたインテリジェントなデジタルエコシステムを構築。

---

<!-- _header: 図6 -->

## 図6: マルチエージェントアーキテクチャ

*(図の内容はMarkdown変換では利用できません)*

*(矢印は可能な通信を表す。ユーザーと専門家モデル間の矢印は明確化のため除外)*

---

<!-- _class: lead -->

# ご清聴ありがとうございました

## AIネイティブメモリ 2.0: Second Me

<br>

**Jiale Wei, Xiang Ying∗, Tao Gao, Fangyi Bao, Felix Tao, Jingbo Shang**
Mindverse.ai
